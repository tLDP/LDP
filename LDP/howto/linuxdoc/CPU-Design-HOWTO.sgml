<!doctype linuxdoc system>
<!-- 

************************** begin comment *****************************
     The following is the HOW-TO for designing the CPU for Linux/Unix OS.
	 This document is in the SGML format. You must use sgml package to 
	 process this document
************************* end of comment *****************************

-->
<!--
************************** SGML USER GUIDE *****************************
	The SGML user guide on linux is located at /usr/doc/sgml-tools
	Read the example.sgml and guide.html documents.
	Usage:
		HTML	  	sgml2html foo  (Do not give extension .sgml here!!)
		Text	 	sgml2txt foo.sgml
		Latex	  	sgml2latex foo.sgml

		Note: Use 2 dashes - before language, error while compiling
		Postscript 	sgml2latex -language=english -o ps foo.sgml
		DVI  		sgml2latex -d foo.sgml
		Lyx		sgml2lyx foo.sgml
		Richtext 	sgml2rtf foo.sgml
		gnuinfo  	sgml2info foo.sgml
		man		sgml2txt -man foo.sgml
		SGML	 	sgmlcheck foo.sgml
************************* end of comment *****************************
-->

<article>

<!-- Title information -->

<title>CPU Design HOW-TO
<!-- chapt change
CPU Design HOW-TO



-->
<author>Al Dev (Alavoor Vasudevan) 
       <htmlurl url="mailto:alavoor@yahoo.com"
             name="alavoor@yahoo.com">
<date>v8.0, 02 Jan 2001
<abstract>
CPU is the "brain" of computer and is a very vital component
of computer system and is like a "cousin brother" of operating system
(Linux or Unix).
This document helps companies, businesses, universities and
research institutes to design, build and manufacture CPUs.
Also the information will be useful for university students of U.S.A and
Canada who are studying computer science/engineering. The document
has URL links which helps students understand how a CPU is designed 
and manufactured. Perhaps in near future there will GNU/GPLed CPU
running Linux, Unix, Microsoft Windows, Apple Mac 
and BeOS operating systems!!
</abstract>

<!-- Table of contents -->
<toc>

<!-- Begin the document -->

<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt>Introduction
-->
<sect>Introduction
<p>
This document provides you comprehensive list of URLs for CPU Design
and fabrication. Using this information students, companies, universities
 or businesses can make new CPUs which can run Linux/Unix operating systems.

In olden days, chip vendors were also the IP developers and the EDA tools
 developers. Nowadays, we have specialized fab companies 
(TSMC <url url="http://www.tsmc.com">), 
IP companies (ARM <url url="http://www.arm.com">,
MIPS <url url="http://www.mips.com">, 
Gray Research LLC <url url="http://cnets.sourceforge.net/grllc.html">
), and tools companies (
Mentor <url url="http://www.mentor.com">, 
Cadence <url url="http://www.cadence.com">, etc.), 
and combinations of 
these (Intel). You can buy IP bundled with hardware (Intel), bundled with
 your tools (EDA 
companies), or separately (IP providers). 

Enter the FPGA vendors (Xilinx <url url="http://www.xilinx.com">, 
Altera <url url="http://www.altera.com">). They have an opportunity to seize
 upon a unique business model. 

VA Linux systems <url url="http://www.valinux.com"> builds the entire 
system and perhaps in future will design and build CPUs for Linux.

Visit the following CPU design sites:
<itemize>
<item> FPGA CPU Links <url url="http://www.fpgacpu.org/links.html">
<item> FPGA Main site <url url="http://www.fpgacpu.org">
<item> OpenRISC 1000 Free Open-source 32-bit RISC processor IP core competing with 
proprietary ARM and MIPS is at <url url="http://www.opencores.org">
<item> Open IP org <url url="http://www.openip.org">
<item> Free IP org - ASIC and FPGA cores for masses <url url="http://www.free-ip.com">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt> What is IP ?
-->
<sect> What is IP ?
<p>
What is IP ? IP is short for <bf>Intellectual Property</bf>. More specifically, it is 
a block of logic that can be used in making ASIC's and FPGA's.  Examples 
of "IP Cores" are, UART's, CPU's, Ethernet Controllers, PCI Interfaces, etc.
In the past, quality cores of this nature could cost anywhere from US$5,000 to
more than US$350,000.  This is way too high for the average company or 
individual to even contemplate using -- Hence, the Free-IP project.

Initially the Free-IP project will focus on the more complex cores, like
CPU's and Ethernet controllers.  Less complex cores might follow.

The Free-IP project is an effort to make quality IP available to anyone.

Visit the following sites for IP cores - 
<itemize>
<item> Open IP org <url url="http://www.openip.org">
<item> Free IP org - ASIC and FPGA cores for masses <url url="http://www.free-ip.com">
<item> FPGA Main site <url url="http://www.fpgacpu.org">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Free CPU List <label id="freecpu">
<p>
Here is the list of Free CPUs available or curently under development -
<itemize>
<item> F-CPU 64-bit Freedom CPU <url url="http://www.f-cpu.org">
mirror site at <url url="http://www.f-cpu.de">
<p>
<item> European Space Agency - SPARC architecture 
LEON CPU <url url="http://www.estec.esa.nl/wsmwww/leon">
<item> European Space Agency - ERC32 SPARC 
V7 CPU <url url="http://www.estec.esa.nl/wsmwww/erc32">
<item> Atmel ERC32 SPARC part # 
TSC695E <url url="http://www.atmel-wm.com/products"> click 
on Aerospace=>Space=>Processors
<p>
<item> Sayuri at <url url="http://www.morphyplanning.co.jp/Products/FreeCPU/freecpu-e.html">
and manufactured by Morphy Planning Ltd at <url url="http://www.morphyone.org">
and feature list at <url url="http://ds.dial.pipex.com/town/plaza/aj93/waggy/hp/features/morphyone.htm"> 
and in Japanese language at <url url="http://www.morphyplanning.or.jp">
<p>
<item> OpenRISC 1000 Free 32-bit processor IP core competing with 
proprietary ARM and MIPS is at <url url="http://www.opencores.org/cores/or1k">
<item> OpenRISC 2000 is at <url url="http://www.opencores.org">
<p>
<item> Green Mountain - GM HC11 CPU Core is at <url url="http://www.gmvhdl.com/hc11core.html">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Commercial CPU List <label id="comcpu">
<p>
<itemize>
<item> ARC CPUs : <url url="http://www.arccores.com">
<item> QED RISC 64-bit and MIPS cpus : <url url="http://www.qedinc.com/about.htm">
<item> Origin 2000 CPU - <url url="http://techpubs.sgi.com/library/manuals/3000/007-3511-001/html/O2000Tuning.1.html">
<item> Hitachi SH4,3,2,1 CPUs <url url="http://semiconductor.hitachi.com/superh">
<item> NVAX CPUs <url url="http://www.digital.com/info/DTJ700">
<item> Univ. of Mich High-perf. GaAs Microprocessor Project <url url="http://www.eecs.umich.edu/UMichMP">
<item> Hyperstone E1-32 RISC/DSP processor <url url="http://bwrc.eecs.berkeley.edu/CIC/tech/hyperstone">
<item> PSC1000 32-bit RISC processor <url url="http://www.ptsc.com/psc1000/index.html">
<item> IDT R/RV4640 and R/RV4650 64-bit CPU w/DSP Capability <url url="http://www.idt.com/products/pages/Processors-PL100_Sub205_Dev128.html">
<item> CPU Info center - List of CPUs sparc, arm etc.. <url url="http://bwrc.eecs.berkeley.edu/CIC/tech">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt> CPU Museum and Silicon Zoo
-->
<sect> CPU Museum and Silicon Zoo
<p>
CPU Museum is at 
<itemize>
<item> Intel CPU Museum <url url="http://www.intel.com/intel/intelis/museum">
<item> Intel - History of Microprocessors <url url="http://www.intel.com/intel/museum/25anniv">
<item> Virtual Museum of Computing <url url="http://www.museums.reading.ac.uk/vmoc">
<item> Silicon Zoo <url url="http://micro.magnet.fsu.edu/creatures/index.html">
<item> Intel - How the Microprocessors work <url url="http://www.intel.com/education/mpuworks">
<item> Simple course in Microprocessors <url url="http://www.hkrmicro.com/course/micro.html">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> How Transistors work <label id="trans">
<p>
      Microprocessors are essential to many of the products we use every day such as TVs, cars, radios, home appliances and of course, computers. Transistors are the main components of microprocessors.
     At their most basic level, transistors may seem simple. But their development actually required many years of painstaking research. Before transistors, computers relied on slow, inefficient vacuum tubes and mechanical switches to process information. In 1958, engineers (one of them Intel founder Robert Noyce) managed to put two transistors onto a silicon crystal and create the first integrated circuit that led to the microprocessor.

    Transistors are miniature electronic switches. They are the building blocks of the microprocessor which is the brain of the computer.
   Similar to a basic light switch, transistors have two operating positions, on and off. This on/off, or binary functionality of transistors enables the processing of information in a computer.
 
<bf>How a simple electronic switch works: </bf>
<p>
    The only information computers understand are electrical signals that are switched on and off. To comprehend transistors, it is necessary to have an understanding of how a switched electronic circuit works.
   Switched electronic circuits consist of several parts. One is the circuit pathway where the electrical current flows - typically through a wire. Another is the switch, a device that starts and stops the flow of electrical current by either completing or breaking the circuit's pathway.
   Transistors have no moving parts and are turned on and off by electrical signals. The on/off switching of transistors facilitates the work performed by microprocessors.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> How a Transistors handles information<label id="transinfo">
<p>
    Something that has only two states, like a transistor, can be referred to as binary. The transistor's on state is represented by a 1 and the off state is represented by a 0. Specific sequences and patterns of 1's and 0's generated by multiple transistors can represent letters, numbers, colors and graphics. This is known as binary notation 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Displaying binary information <label id="bininfo">
<p>
<bf> Spell your name in Binary: </bf>
<p>
    Each character of the alphabet has a binary equivalent. Below is the name JOHN and its equivalent in binary. 
<code>
	J  0100 1010
	O  0100 1111
	H  0100 1000
	N  0100 1110
</code>
	More complex information can be created such as graphics, audio and video using the binary, or on/off action of transistors.

   Scroll down to the Binary Chart below to see the complete alphabet in binary.
 
<!-- Put a line space after &lowbar below to avoid space between....  -->
<table loc=p>
<tabular ca="rll">
Character <colsep>Binary <colsep>Character <colsep>Binary <rowsep><hline> 
A <colsep> 0100 0001 <colsep> N <colsep> 0100 1110 <rowsep> 
B <colsep> 0100 0010 <colsep> O <colsep> 0100 1111 <rowsep> 
C <colsep> 0100 0011 <colsep> P <colsep> 0101 0000 <rowsep> 
D <colsep> 0100 0100 <colsep> Q <colsep> 0101 0001 <rowsep> 
E <colsep> 0100 0101 <colsep> R <colsep> 0101 0010 <rowsep> 
F <colsep> 0100 0110 <colsep> S <colsep> 0101 0011 <rowsep> 
G <colsep> 0100 0111 <colsep> T <colsep> 0101 0100 <rowsep> 
H <colsep> 0100 1000 <colsep> U <colsep> 0101 0101 <rowsep> 
I <colsep> 0100 1001 <colsep> V <colsep> 0101 0110 <rowsep> 
J <colsep> 0100 1010 <colsep> W <colsep> 0101 0111 <rowsep> 
K <colsep> 0100 1011 <colsep> X <colsep> 0101 1000 <rowsep> 
L <colsep> 0100 1100 <colsep> Y <colsep> 0101 1001 <rowsep> 
M <colsep> 0100 1101 <colsep> Z <colsep> 0101 1010 <rowsep>
</tabular>
<caption><bf>Binary Chart for Alphabets</bf></caption>
</table>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> What is a Semi-conductor? <label id="semicon">
<p>
Conductors and insulators : 

    Many materials, such as most metals, allow electrical current to flow through them. These are known as conductors. Materials that do not allow electrical current to flow through them are called insulators. Pure silicon, the base material of most transistors, is considered a semiconductor because its conductivity can be modulated by the introduction of impurities.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect2> Anatomy of Transistor <label id="anatomy">
<p>
Semiconductors and flow of electricity 

    Adding certain types of impurities to the silicon in a transistor changes its crystalline structure and enhances its ability to conduct electricity. Silicon containing boron impurities is called p-type silicon - p for positive or lacking electrons. Silicon containing phosphorus impurities is called n-type silicon - n for negative or having a majority of free electrons 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect2> A Working Transistor <label id="worktrans">
<p>
A Working transistor - The On/Off state of Transistor

    Transistors consist of three terminals; the source, the gate and the drain.  

    In the n-type transistor, both the source and the drain are negatively-charged and sit on a positively-charged well of p-silicon.  

    When positive voltage is applied to the gate, electrons in the p-silicon are attracted to the area under the gate forming an electron channel between the source and the drain.  

    When positive voltage is applied to the drain, the electrons are pulled from the source to the drain. In this state the transistor is on.  

    If the voltage at the gate is removed, electrons aren't attracted to the area between the source and drain. The pathway is broken and the transistor is turned off.  
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect2> Impact of Transistors <label id ="impact">
<p>
The Impact of Transistors - How microprocessors affect our lives. 

     The binary function of transistors gives micro- processors the ability to perform many tasks; from simple word processing to video editing. Micro- processors have evolved to a point where transistors can execute hundreds of millions of instructions per second on a single chip.
   Automobiles, medical devices, televisions, computers and even the Space Shuttle use microprocessors. They all rely on the flow of binary information made possible by the transistor.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt> CPU Design and Architecture <label id="cpudesign">
-->
<sect> CPU Design and Architecture <label id="cpudesign">
<p>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> CPU Design <label id="cpudesign">
<p>
Visit the following links for information on CPU Design.
<itemize>
<item> Hamburg University VHDL archive <url url="http://tech-www.informatik.uni-hamburg.de/vhdl">
<item> Kachina Design tools <url url="http://SAL.KachinaTech.COM/Z/1/index.shtml">
<item> List of FPGA-based Computing Machines <url url="http://www.io.com/~guccione/HW_list.html">
<item> SPARC International <url url="http://www.sparc.com">
<item> Design your own processor <url url="http://www.spacetimepro.com">
<item> Teaching Computer Design with FPGAs <url url="http://www.fpgacpu.org">
<item> Technical Committee on Computer Architecture <url url="http://www.computer.org/tab/tcca">
<p>
<item> Frequently Asked Questions FAQ on VHDL <url url="http://www.vhdl.org/vi/comp.lang.vhdl">
or it is at <url url="http://www.vhdl.org/comp.lang.vhdl">
<item> Comp arch FAQ <url url="http://www.esacademy.com/automation/faq.htm">
<item> Comp arch FAQ <url url="ftp://rtfm.mit.edu/pub/usenet-by-hierarchy/comp/arch">
<item> VME Bus FAQ <url url="http://www.hitex.com/automation/FAQ/vmefaq">
<p>
<item> Homepage of SPEC <url url="http://performance.netlib.org/performance/html/spec.html">
<item> Linux benchmarks <url url="http://www.silkroad.com/linux-bm.html">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************


-->
<sect1> Online Textbooks on CPU Architecture <label id="books">
<p>
<itemize>
<item> Online HTML book <url url="http://odin.ee.uwa.edu.au/~morris/CA406/CA_ToC.html">
<item> Univ of Texas Comp arch : <url url="http://www.cs.panam.edu/~meng/Course/CS4335/Notes/master/master.html">
<item> Number systems and Logic circuits : <url url="http://www.tpub.com/neets/book13/index.htm">
<item> Digital Logic: <url url="http://www.play-hookey.com/digital">
<item> FlipFlops: <url url="http://www.ece.utexas.edu/~cjackson/FlipFlops/web_pages/Publish/FlipFlops.html">
<item> Instruction Execution cycle: <url url="http://cq-pan.cqu.edu.au/students/timp1/exec.html">
<item> Truth Table constructor: <url url="http://pirate.shu.edu/~borowsbr/Truth/Truth.html">
<item> Overview of Shared Memory: <url url="http://www.sics.se/cna/mp_overview.html">
<item> Simulaneous Multi-threading in processors : <url url="http://www.cs.washington.edu/research/smt">
<item> Study Web : <url url="http://www.studyweb.com/links/277.html">
<item> Univ notes: <url url="http://www.ece.msstate.edu/~linder/Courses/EE4713/notes">
<item> Advice: An Adaptable and Extensible Distributed Virtual Memory Architecture <url url="http://www.gsyc.inf.uc3m.es/~nemo/export/adv-pdcs96/adv-pdcs96.html">
<item> Univ of Utah Avalanche Scalable Parallel Processor Project <url url="http://www.cs.utah.edu/avalanche/avalanche-publications.html">
<item> Distributed computing : <url url="http://www.geocities.com/SiliconValley/Vista/4015/pdcindex.html">
<item> Pisma Memory architecture: <url url="http://aiolos.cti.gr/en/pisma/pisma.html">
<item> Shared Mem Arch: <url url="http://www.ncsa.uiuc.edu/General/Exemplar/ARPA">
<item> Textbooks on Comp Arch: <url url="http://www.rdrop.com/~cary/html/computer_architecture.html#book">
<p>
<item> Comp Arch Conference and Journals <url url="http://www.handshake.de/user/kroening/conferences.html">
<item> WWW Comp arch page <url url="http://www.cs.wisc.edu/~arch/www">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************


-->
<sect1> University Lecture notes on CPU Architecture <label id="univlectures">
<p>
<itemize>
<item> Advanced Computer Architecture 
<url url="http://www.cs.utexas.edu/users/dahlin/Classes/GradArch">
<item> Computer architecture - Course level 415 <url url="http://www.diku.dk/teaching/2000f/f00.415">
<item> MIT: <url url="http://www.csg.lcs.mit.edu/6.823">
<item> UBC CPU slides : <url url="http://www.cs.ubc.ca/spider/neufeld/courses/cs218/chapter8/index.htm">
<item> Purdue Univ slides: <url url="http://www.ece.purdue.edu/~gba/ee565/Sessions/S03HTML/index.htm">
<item> Rutgers Univ - Principles of Comp Arch : <url url="http://www.cs.rutgers.edu/~murdocca/POCA/Chapter02.html">
<item> Brown Univ - <url url="http://www.engin.brown.edu/faculty/daniels/DDZO/cmparc.html">
<item> Univ of Sydney - Intro Digital Systems : <url url="http://www.eelab.usyd.edu.au/digital_tutorial/part3">
<item> Bournemouth Univ, UK Principles of Computer Systems : <url url="http://ncca.bournemouth.ac.uk/CourseInfo/BAVisAn/Year1/CompSys">
<item> Parallel Virtual machine: <url url="http://www.netlib.org/pvm3/book/node1.html">
<item> univ center: <url url="http://www.eecs.lehigh.edu/~mschulte/ece401-99">
<item> univ course: <url url="http://www.cs.utexas.edu/users/fussell/cs352">
<item> Examples of working VLSI circuits(in Greek) <url url="http://students.ceid.upatras.gr/~gef/projects/vlsi">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************


-->
<sect1> CPU Architecture <label id="cpuarch">
<p>
Visit the following links for information on CPU architecture
<itemize>
<item> Comp architecture: <url url="http://www.rdrop.com/~cary/html/computer_architecture.html">
<item> Beyond RISC - The Post-RISC Architecture <url url="http://www.cps.msu.edu/~crs/cps920">
<item> Beyond RISC - PostRISC : <url url="http://www.ceng.metu.edu.tr/~e106170/postrisc.html">
<item> List of CPUS <url url="http://einstein.et.tudelft.nl/~offerman/cl.contents2.html">
<item> PowerPC Arch <url url="http://www.mactech.com/articles/mactech/Vol.10/10.08/PowerPcArchitecture">
<item> CPU Info center - List of CPUs sparc, arm etc.. <url url="http://bwrc.eecs.berkeley.edu/CIC/tech">
<item> cpu arch intel IA 64 <url url="http://developer.intel.com/design/ia-64">
<item> Intel 386 CPU architecture <url url="http://www.delorie.com/djgpp/doc/ug/asm/about-386.html">
<item> Freedom CPU architecture <url url="http://f-cpu.tux.org/original/Freedom.php3">
<item> Z80 CPU architecture <url url="http://www.geocities.com/SiliconValley/Peaks/3938/z80arki.htm">
<item> CRIMSEN OS and teaching-aid CPU<url url="http://www.dcs.gla.ac.uk/~ian/project3/node1.html">
<item> Assembly Language concepts <url url="http://www.cs.uaf.edu/~cs301/notes/Chapter1/node1.html">
<item> Alpha CPU architecture <url url="http://www.linux3d.net/cpu/CPU/alpha/index.shtml">
<item> <url url="http://hugsvr.kaist.ac.kr/~exit/cpu.html">
<item> Tron CPU architecture <url url="http://tronweb.super-nova.co.jp/tronvlsicpu.html">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Usenet Newsgroups for CPU design <label id="newsgroups">
<p>
<itemize>
<item> Newsgroup computer architecture <url url="news:comp.arch">
<item> Newsgroup FPGA <url url="news:comp.arch.fpga">
<item> Newsgroup Arithmetic <url url="news:comp.arch.arithmetic">
<item> Newsgroup Bus <url url="news:comp.arch.bus">
<item> Newsgroup VME Bus <url url="news:comp.arch.vmebus">
<item> Newsgroup embedded <url url="news:comp.arch.embedded">
<item> Newsgroup embedded piclist <url url="news:comp.arch.embedded.piclist">
<item> Newsgroup storage <url url="news:comp.arch.storage">
<item> Newsgroup VHDL <url url="news:comp.lang.vhdl">
<item> Newsgroup Computer Benchmarks <url url="news:comp.benchmarks">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt> Fabrication, Manufacturing CPUs <label id="fabricate">
-->
<sect> Fabrication, Manufacturing CPUs <label id="fabricate">
<p>
After doing the design and testing of CPU, your company may want to mass produce 
the CPUs. There are many "semi-conductor foundries" in the world who will do
that for you for a nominal competetive cost. There are companies in USA, 
Germany, UK, Japan, Taiwan, Korea and China. 

TMSC (Taiwan) is the <bf>"largest independent foundry"</bf> in the world. 
You may want to shop around and you will get the best rate 
for a very high volume production (greater than 100,000 CPU units).
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Foundry Business is in Billions of dollars!!
<p>
Foundry companies invested very heavily in the infra-structure
and building plants runs in several millions of dollars!
Silicon foundry business will grow from $7 billion to $36 
billion by 2004 (414% increase!!).
More integrated device manufacturers (IDMs) opt to outsource
chip production verses adding wafer-processing capacity.

Independent foundries currently produce about 12% of the semiconductors 
in the world, and by 2004, that share will more than double to 26%. 

The "Big Three" pure-play foundries are -- Taiwan 
Semiconductor Manufacturing Co. (TSMC),
United Microelectronics Corp. (UMC), 
and Chartered Semiconductor Manufacturing Ltd. Pte.--collectively 
account for 69% of today's silicon foundry volume, but 
their share is expected to grow to 88% by 2004.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Fabrication of CPU
<p>
There are hundreds of foundries in the world (too numerous to list). Some of them are -
<itemize>
<item> TSMC (Taiwan Semi-conductor Manufacturing Co) <url url="http://www.tsmc.com">, 
about co <url url="http://www.tsmc.com/about/index.html">
<item> Chartered Semiconductor Manufacturing, Singapore <url url="http://www.csminc.com">
<item> United Microelectronics Corp. (UMC) <url url="http://www.umc.com/index.html">
<item> Advanced BGA Packing <url url="http://www.abpac.com">
<item> Amcor, Arizona <url url="http://www.amkor.com">
<item> Elume, USA <url url="http://www.elume.com">
<item> X-Fab, Gesellschaft zur Fertigung von Wafern mbH, Erfurt, Germany <url url="http://www.xfab.com">
<item> IBM corporation, (Semi-conductor foundry div) <url url="http://www.ibm.com">
<item> National Semi-conductor Co, Santa Clara, USA <url url="http://www.natioanl.com">
<item> Intel corporation (Semi-conductor foundries), USA <url url="http://www.intel.com">
<item> Hitachi Semi-conductor Co, Japan <url url="http://www.hitachi.com">
<item> Fujitsu Semi-conductor Co, Japan
<item> Mitsubhishi Semi-conductor Co, Japan
<item> Hyandai Semi-conductor, Korea <url url="http://www.hea.com">
<item> Samsumg Semi-conductor, Korea
<item> Atmel, France <url url="http://www.atmel-wm.com">
</itemize>
If you know any major foundries, let me know I will add to list.

List of CHIP foundry companies 
<itemize>
<item> Chip directory <url url="http://www.xs4all.nl/~ganswijk/chipdir/make/foundry.htm">
<item> Chip makers <url url="http://www.xs4all.nl/~ganswijk/chipdir/make/index.htm">
<item> IC manufacturers <url url="http://www.xs4all.nl/~ganswijk/chipdir/c/a.htm">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt change> Super Computer Architecture
-->
<sect> Super Computer Architecture
<p>
For building Super computers, the trend that seems to emerge is 
that most new systems look as minor 
variations on the same theme: clusters of RISC-based Symmetric 
Multi-Processing (SMP) nodes which in turn are connected by a fast
network. Consider this as a natural architectural evolution. 
The availability of relatively low-cost (RISC) processors and 
network products to connect these processors together with 
standardised communication software has stimulated the building 
of home-brew clusters computers as an alternative to complete 
systems offered by vendors.

Visit the following sites for Super Computers -
<itemize>
<item> Top 500 super computers <url url="http://www.top500.org/ORSC/2000">
<item> National Computing Facilities Foundation <url url="http://www.nwo.nl/ncf/indexeng.htm">
<item> Linux Super Computer Beowulf cluster <url url="http://www.linuxdoc.org/HOWTO/Beowulf-HOWTO.html">
<item> Extreme machines - beowulf cluster <url url="http://www.xtreme-machines.com">
<item> System architecture description of
the Hitachi SR2201 <url url="http://www.hitachi.co.jp/Prod/comp/hpc/eng/sr1.html">
<item> Personal Parallel Supercomputers <url url="http://www.checs.net/checs_98/papers/super">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> Main Architectural Classes
<p>
Before going on to the descriptions of the machines themselves, it is 
important to consider some mechanisms that are or have been used to 
increase the performance. The hardware structure or architecture 
determines to a large extent what the possibilities and impossibilities 
are in speeding up a computer system beyond the performance of a single 
CPU. Another important factor that is considered in combination with 
the hardware is the capability of compilers to generate efficient code
to be executed on the given hardware platform. In many cases it is hard
to distinguish between hardware and software influences and one has to be 
careful in the interpretation of results when ascribing certain effects 
to hardware or software peculiarities or both. In this chapter we will 
give most emphasis to the hardware architecture. For a description of
machines that can be considered to be classified as "high-performance".

Since many years the taxonomy of Flynn has proven to be useful for
the classification of high-performance computers. This classification 
is based on the way of manipulating of instruction and data streams and 
comprises four main architectural classes. We will first briefly sketch 
these classes and afterwards fill in some details when each of the 
classes is described. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> SISD machines 
<p>
These are the conventional systems that contain one CPU 
and hence can accommodate one instruction stream that is executed serially.
Nowadays many large mainframes may have more than one CPU but each of 
these execute instruction streams that are unrelated. Therefore, such 
systems still should be regarded as (a couple of) SISD machines acting 
on different data spaces. Examples of SISD machines are for instance 
most workstations like those of DEC, Hewlett-Packard, and Sun 
Microsystems. The definition of SISD machines is given here for 
completeness' sake. We will not discuss this type of machines 
in this report. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> 
SIMD machines
<p>
Such systems often have a large number of processing 
units, ranging from 1,024 to 16,384 that all may execute the same 
instruction on different data in lock-step. So, a single instruction 
manipulates many data items in parallel. Examples of SIMD machines 
in this class are the CPP DAP Gamma II and the Alenia Quadrics. 

Another subclass of the SIMD systems are the vectorprocessors. 
Vectorprocessors act on arrays of similar data rather than on single 
data items using specially structured CPUs. When data can be manipulated
by these vector units, results can be delivered with a rate of one,
two and --- in special cases --- of three per clock cycle (a clock 
cycle being defined as the basic internal unit of time for the system).
So, vector processors execute on their data in an almost parallel way
but only when executing in vector mode. In this case they are several
times faster than when executing in conventional scalar mode. For 
practical purposes vectorprocessors are therefore mostly regarded 
as SIMD machines. Examples of such systems is for instance 
the Hitachi S3600. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> 
MISD machines
<p>
Theoretically in these type of machines multiple 
instructions should act on a single stream of data. As yet no
practical machine in this class has been constructed nor are 
such systems easily to conceive. We will disregard them in the 
following discussions. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> 
MIMD machines
<p>
These machines execute several instruction 
streams in parallel on different data. The difference with the
multi-processor SISD machines mentioned above lies in the fact that 
the instructions and data are related because they represent different 
parts of the same task to be executed. So, MIMD systems may run 
many sub-tasks in parallel in order to shorten the time-to-solution
for the main task to be executed. There is a large variety of 
MIMD systems and especially in this class the Flynn taxonomy proves 
to be not fully adequate for the classification of systems. Systems 
that behave very differently like a four-processor NEC SX-5 and a
thousand processor SGI/Cray T3E fall both in this class. In the 
following we will make another important distinction between classes 
of systems and treat them accordingly. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect2> 
Shared memory systems
<p>
Shared memory systems have multiple CPUs all
of which share the same address space. This means that the knowledge 
of where data is stored is of no concern to the user as there is only
one memory accessed by all CPUs on an equal basis. Shared memory 
systems can be both SIMD or MIMD. Single-CPU vector processors can be 
regarded as an example of the former, while the multi-CPU models of 
these machines are examples of the latter. We will sometimes use the 
abbreviations SM-SIMD and SM-MIMD for the two subclasses. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect2> 
Distributed memory systems
<p>
In this case each CPU has its own
associated memory. The CPUs are connected by some network and may
exchange data between their respective memories when required. In 
contrast to shared memory machines the user must be aware of the
location of the data in the local memories and will have to move
or distribute these data explicitly when needed. Again, distributed 
memory systems may be either SIMD or MIMD. The first class of
SIMD systems mentioned which operate in lock step, all have distributed
memories associated to the processors. As we will see, 
distributed-memory MIMD systems exhibit a large variety in the 
topology of their connecting network. The details of this topology 
are largely hidden from the user which is quite helpful with 
respect to portability of applications. For the distributed-memory 
systems we will sometimes use DM-SIMD and DM-MIMD to indicate
the two subclasses. 
Although the difference between shared- and distributed memory 
machines seems clear cut, this is not always entirely the case
from user's point of view. For instance, the late Kendall 
Square Research systems employed the idea of "virtual shared memory" 
on a hardware level. Virtual shared memory can also be simulated 
at the programming level: A specification of High Performance 
Fortran (HPF) was published in 1993 which by means of 
compiler directives distributes the data over the 
available processors. Therefore, the system on which HPF is
implemented in this case will look like a shared memory machine 
to the user. Other vendors of Massively Parallel Processing 
systems (sometimes called MPP systems), like HP 
and SGI/Cray,
also are able to support proprietary virtual shared-memory programming models due to 
the fact that these physically distributed memory systems are able to address 
the whole collective address space. So, for 
the user such systems have one global address space spanning all of 
the memory in 
the system. We will say a little more about 
the structure of such systems in 
the ccNUMA section. In addition, packages like TreadMarks
provide a virtual shared memory environment for networks of workstations. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> 
Distributed Processing Systems
<p>
Another trend that has came up in 
the last few years is distributed processing. This takes 
the DM-MIMD concept one step further: instead 
of many integrated processors in one or several boxes, 
workstations, mainframes, etc., are connected by (Gigabit) Ethernet, FDDI, or otherwise 
and set to work concurrently on tasks in
the same program. Conceptually, this is not different from DM-MIMD computing, but
the communication between processors is often orders 
of magnitude slower. Many packages to realise distributed 
computing are available. Examples of
these are PVM (st
anding for Parallel Virtual Machine), 
and MPI (Message Passing Interface). This style 
of programming, called
the "message passing" model has becomes so much accepted that PVM 
and MPI have been adopted by virtually all major vendors 
of distributed-memory MIMD systems 
and even on shared-memory MIMD systems for compatibility reasons. In addition
there is a tendency to cluster shared-memory systems, 
for instance by HiPPI channels, to obtain systems 
with a very high computational power. E.g.,
the NEC SX-5, 
and
the SGI/Cray SV1 have this structure. So, within
the clustered nodes a shared-memory programming style can be 
used while between clusters message-passing should be used. 
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************

-->
<sect1> 
ccNUMA machines
<p>
As already mentioned in the introduction, a trend can be 
observed to build systems that have a rather small (up to 16)
number of RISC processors that are tightly integrated in 
a cluster, a Symmetric Multi-Processing (SMP) node. The
processors in such a node are virtually always connected 
by a 1-stage crossbar while these clusters are connected by a
less costly network. 

This is similar to the policy mentioned for large 
vectorprocessor ensembles mentioned above but with the important
difference that all of the processors can access all of the 
address space. Therefore, such systems can be considered as
SM-MIMD machines. On the other hand, because the memory is 
physically distributed, it cannot be guaranteed that a
data access operation always will be satisfied within the same 
time. Therefore such machines are called ccNUMA
systems where ccNUMA stands for Cache Coherent Non-Uniform Memory 
Access. The term "Cache Coherent" refers
to the fact that for all CPUs any variable that is to be used
must have a consistent value. Therefore, is must be assured
that the caches that provide these variables are also consistent 
in this respect. There are various ways to ensure that the
caches of the CPUs are coherent. One is the snoopy bus 
protocol in which the caches listen in on transport of variables to
any of the CPUs and update their own copies of these 
variables if they have them. Another way is the directory memory,
a special part of memory which enables to keep track of the all 
copies of variables and of their validness. 

For all practical purposes we can classify these systems as 
being SM-MIMD machines also because special assisting
hardware/software (such as a directory memory) has been
incorporated to establish a single system image although
the memory is physically distributed.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt change> Neural Network Processors
-->
<sect> Neural Network Processors
<p>
NNs are models of biological neural networks and some are not, but 
historically, much of the inspiration for the field of
NNs came from the desire to produce artificial systems capable 
of sophisticated, perhaps "intelligent", computations similar to
those that the human brain routinely performs, and thereby 
possibly to enhance our understanding of the human brain. 

Most NNs have some sort of "training" rule whereby the weights
of connections are adjusted on the basis of data. In other
words, NNs "learn" from examples (as children learn to 
recognize dogs from examples of dogs) and exhibit some capability for
generalization beyond the training data. 

NNs normally have great potential for parallelism, since the computations 
of the components are largely independent of each
other. Some people regard massive parallelism and high connectivity to 
be defining characteristics of NNs, but such
requirements rule out various simple models, such as simple 
linear regression (a minimal feedforward net with only two units
plus bias), which are usefully regarded as special cases of NNs.

Some definitions of Neural Network (NN) are as follows:
<itemize>
<item> According to the DARPA Neural Network Study :
A neural network is a system composed of many simple processing 
elements operating in parallel whose function is
determined by network structure, connection strengths, and the 
processing performed at computing elements or nodes.
<item> According to Haykin: A neural network is a massively parallel distributed processor that has a natural propensity for storing experiential
knowledge and making it available for use. It resembles the brain in two respects: 
	<itemize>
	<item>Knowledge is acquired by the network through a learning process. 
	<item>Interneuron connection strengths known as synaptic weights are used to store the knowledge. 
	</itemize>
<item> According to Nigrin: A neural network is a circuit composed of a 
very large number of simple processing elements that are neurally based.
Each element operates only on local information. Furthermore each 
element operates asynchronously; thus there is no
overall system clock. 
<item> According to Zurada: Artificial neural systems, or neural networks, are physical cellular systems which can acquire, store, and utilize
experiential knowledge.
</itemize>

Visit the following sites for more info on Neural Network Processors
<itemize>
<item> Omers Neural Network pointers <url url="http://www.cs.cf.ac.uk/User/O.F.Rana/neural.html">
<item> FAQ site <url url="ftp://ftp.sas.com/pub/neural/FAQ.html">
<item> Automation corp <url name="Neural Network Processor" url="http://www.accurate-automation.com/Products/NNP.HTM"> hardware
</itemize>
<!-- 
*******************************************
************ End of Section ***************
*******************************************




<chapt change> Related URLs
-->
<sect> Related URLs
<p>
Visit following locators which are related -
<itemize>
<item> Color Vim editor <url url="http://metalab.unc.edu/LDP/HOWTO/Vim-HOWTO.html">
<item> Source code control system <url url="http://metalab.unc.edu/LDP/HOWTO/CVS-HOWTO.html">
<item> Linux goodies main site <url url="http://www.aldev.8m.com">
<item> Linux goodies mirror site <url url="http://aldev.webjump.com">
<item> Linux goodies mirror site <url url="http://aldev.50megs.com">
</itemize>
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt change> Other Formats of this Document
-->
<sect> Other Formats of this Document
<p>
This document is published in 12 different formats namely - DVI, Postscript, 
Latex, Adobe Acrobat PDF,
LyX, GNU-info, HTML, RTF(Rich Text Format), Plain-text, Unix man pages, single 
HTML file and SGML.
<itemize>
<item>
You can get this HOWTO document as a single file tar ball in HTML, DVI, 
Postscript or SGML formats from -
<url url="ftp://sunsite.unc.edu/pub/Linux/docs/HOWTO/other-formats/">

<item>Plain text format is in: <url url="ftp://sunsite.unc.edu/pub/Linux/docs/HOWTO">

<item>Translations to other languages like French, German, Spanish, 
Chinese, Japanese are in
<url url="ftp://sunsite.unc.edu/pub/Linux/docs/HOWTO">
Any help from you to translate to other languages is welcome.
</itemize>
The document is written using a tool called "SGML-Tools" which can be got from - 
<url url="http://www.sgmltools.org">
Compiling the source you will get the following commands like
<itemize>
<item>sgml2html CPU-Design-HOWTO.sgml     (to generate html file)
<item>sgml2rtf  CPU-Design-HOWTO.sgml     (to generate RTF file)
<item>sgml2latex CPU-Design-HOWTO.sgml    (to generate latex file)
</itemize>

LaTeX documents may be converted into PDF files simply by 
producing a Postscript output using <bf>sgml2latex</bf> ( and dvips) and running the
output through the Acrobat <bf>distill</bf> (<url url="http://www.adobe.com">) command as follows: 
<code>
bash$ man sgml2latex
bash$ sgml2latex filename.sgml
bash$ man dvips
bash$ dvips -o filename.ps filename.dvi
bash$ distill filename.ps
bash$ man ghostscript
bash$ man ps2pdf
bash$ ps2pdf input.ps output.pdf
bash$ acroread output.pdf &
</code>
Or you can use Ghostscript command <bf>ps2pdf</bf>.
ps2pdf is a work-alike for nearly all the functionality of 
Adobe's Acrobat Distiller product: it
converts PostScript files to Portable Document Format (PDF) files. 
<bf>ps2pdf</bf> is implemented as a very small command script (batch file) that invokes Ghostscript, selecting a special "output device"
called <bf>pdfwrite</bf>. In order to use ps2pdf, the pdfwrite device must be included in the makefile when Ghostscript was compiled;
see the documentation on building Ghostscript for details.

This howto document is located at -
<itemize>
<item> <url url="http://sunsite.unc.edu/LDP/HOWTO/CPU-Design-HOWTO.html">
</itemize>

Also you can find this document at the following mirrors sites -
<itemize>
<item> <url url="http://www.caldera.com/LDP/HOWTO/CPU-Design-HOWTO.html">
<item> <url url="http://www.WGS.com/LDP/HOWTO/CPU-Design-HOWTO.html">
<item> <url url="http://www.cc.gatech.edu/linux/LDP/HOWTO/CPU-Design-HOWTO.html">
<item> <url url="http://www.redhat.com/linux-info/ldp/HOWTO/CPU-Design-HOWTO.html">

<item> Other mirror sites near you (network-address-wise) can be found at
<url url="http://sunsite.unc.edu/LDP/hmirrors.html">
select a site and go to directory /LDP/HOWTO/CPU-Design-HOWTO.html
</itemize>


In order to view the document in dvi format, use the xdvi program. The xdvi
program is located in tetex-xdvi*.rpm package in Redhat Linux which can be
located through ControlPanel | Applications | Publishing | TeX menu buttons.
	To read dvi document give the command -
<tscreen><verb>
	xdvi -geometry 80x90 howto.dvi
	man xdvi
</verb></tscreen>
	And resize the window with mouse.
	To navigate use Arrow keys, Page Up, Page Down keys, also
	you can use 'f', 'd', 'u', 'c', 'l', 'r', 'p', 'n' letter
	keys to move up, down, center, next page, previous page etc.
	To turn off expert menu press 'x'.

You can read postscript file using the program 'gv' (ghostview) or 
'ghostscript'.
The ghostscript program is in ghostscript*.rpm package and gv 
program is in gv*.rpm package in Redhat Linux
which can be located through ControlPanel | Applications | Graphics menu 
buttons. The gv program is much more user friendly than ghostscript.
Also ghostscript and gv are available on other platforms like OS/2,
Windows 95 and NT, you view this document even on those platforms.
 
<itemize>
<item>Get ghostscript for Windows 95, OS/2, and for all OSes from <url url="http://www.cs.wisc.edu/~ghost">
</itemize>

To read postscript document give the command -
<tscreen><verb>
		gv howto.ps
		ghostscript howto.ps
</verb></tscreen>

You can read HTML format document using Netscape Navigator, Microsoft Internet
explorer, Redhat Baron Web browser or any of the 10 other web browsers.

You can read the latex, LyX output using LyX a X-Windows front end to latex.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




<chapt> Copyright
-->
<sect> Copyright
<p>
Copyright policy is GNU/GPL as per LDP (Linux Documentation project).
LDP is a GNU/GPL project.
Additional restrictions are - you must retain the author's name, email address
and this copyright notice on all the copies. If you make any changes 
or additions to this document then you should 
intimate all the authors of this document.
<!-- 
	*******************************************
	************ End of Section ***************
	*******************************************




-->
</article>

