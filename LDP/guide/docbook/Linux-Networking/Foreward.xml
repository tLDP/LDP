<sect1 id="Foreward">

<title>Foreward</title>

A computer network is a group of two or more computers that communicate 
through a transmission medium. Their main purpose is to share resources, 
both physical and logical, such as files, devices (such as printers and 
modems) and services. 

They could be thought though of being consisting of two fundamental things, devices and 
connections. However, this will not be of any real practical use since there is no way
in which for humans to use the network to receive or distribute data.

Hence, the basic neccessities for a successful and useful computer network are a
communication link, an interface to the link and software to access the link.
NB - Throughtout this document, the words "link" and "connection" are loosely synonymous.

The theory behind sending and receiving information is all well and good but how does 
it actually occur especially in the case of communication between more than two parties?

Well, firstly, communication can take a number of forms in simplex, half-duplex and duplex. 
In the case of simplex, this form of communication is when only one party can only 
listen and the other can only send. In the case of half-duplex, each party must be 
listening or sending but not both. Finally, duplex or full-duplex communication 
occurs when both parties can receive and send information at the same time.

Secondly, a common standard must be agreed to as to who will take precedence. ie. a 
"common language" to determine whether there should be priority levels on messages 
(in case there is an emergency), what type of error correction should occur (ie. whether 
a simple mechanism such as even-parity will suffice of whether something more 
sophisticated is required such as CRC-32), what should happen when an error in 
communication does occur, etc.... In other words, a protocol must be formed.
In the network setting, a protocol is an agreement between communicating parties/entities 
on how communication is to progress. 

Thirdly, it has been generally found that the most efficient and least error prone 
method by which to achieve these goals is through 'layering'. This idea will be explained 
further on in the section entitled the OSI and TCP/IP network layering models but it basically involves 
abstracting information starting from the top where humans see it (messages) and breaking it down
(ie. datagrames, packets, frames, cells and finally bits). As the information is sent up or down
further information will be removed or added respectively to a 'header' which now forms a part of the 
original data. It is this header which contains the information necessary for error correction, message
prioritisation, etc....

Using a virtual machine concept, each layer is a virtual machine, with layer one being the 
real machine. The top layer provides the highest level functionality or the functions that 
are most abstracted from the physical world. The top layer is directly interpreted by human beings.
The bottom layer provides the lowest level functionality, ie. it is strongly related to the 
physical world and (preferably) is not interpreted by human beings.

In a layered model, entities forming the corresponding layers on different machines are 
called peers and protocols forms a central part of network software. The layered approach 
to networks and general software engineering principles adopted add to the structure 
of network software. Each layer performs a small set of well defined functions (services) 
required by the layer above it.

The layered approach offers a communication setting where layer n on one machine can have a 
conversation with layer n on the other mahine. Layer n-protocol is essentially a set of rules 
and conventions facilitating this conversation. This includes addressing and specification of 
necessary DU's (Data Units).

You should note that this communication between layers is virtual. There is no physical or 
direct communication between layers of two layer-n hosts. The actual communication takes 
place at the lowest layer (usually called the physical layer). The conglomeration of layers 
and corresponding layer protocols form a network architecture.

The general consensus in computing is that a typical data unit exchanged between systems 
should consist of the address of the transmitting computer, the address of the receiving 
computer, the actual data being transmitted, as well as a checksum.

This leads us to the problem of addressing. In order for computers to communicate properly 
it was generally agreed by Ethernet card manufacturers that all NIC cards would possess a 
48 bit unique address. This is called a MAC address but is often called the hardware address 
of these cards. This aids portability and modularity of LAN (Local Area Network) technology and 
software to a major extent. The data units here are called as frames. This is all you need 
really to have a small network.

However, there exists a fundamental problem if you were to extend this idea to larger systems 
(ie. greater than 100 nodes). It is extremely difficult to keep track of and maintain such a 
network due to administrators having to keep track of the name of each and every system and 
deciding what the name of new computers on the network will be.

For this reason, the idea of hostnames and network addresses were developed. For example, 
on the LAN a computer may be called "computer" but on the internet it may be referred to 
as "computer.network.com". The idea behind network addressing came to be known simply now 
as IP (Internet Prococol) addressing.

You could say that the idea behind computer network addressing is roughly synonymous with that 
of the rather mundane telephone network. To call a number in your region all you have to do 
is dial that number. To call a number in another state you must add a number of other digits 
to the start of the number. To call a number that is overseas you must add further digits 
to the beginning of the now burgeoning number. The only difference between telephone and 
network addressing is that you add numbers to the front rather than at the end of the address.

To this day, it has been found that by utilising so called layer architecture for networks, 
suitable protocols and appropriate communication technologies the issues of network
application interfacing, network addressing and network functionality can be addressed
successfully.

There are eight main network technology issues that must be addressed at each layer in the
architecture though. These are outlined below:

1. Mechanism of identifying senders and receivers: addressing.
2. Rules for data transfer: simplex, half-duplex, or full-duplex.
3. Logical channels: sharing a link among a number of connections.
4. Error control strategies: correction and detection.
5. Sequencing protocols for the correct order of messages: put messages in the correct order.
6. Incompatible speed between fast sender and slow receiver.
7. Message fragmentation and assembly.
8. Strategies for choosing routes.

To study the above issues in detail please consult, Tannenbaum 4th edition.

These design issues become recurring themes that are usually addressed by each and every 
layer in the architecture. As a stark example, although error detection and correction is 
undertaken by the low level transmission protocol that sends characters from a terminal 
to the display, the user will also implement error detection and correction at the highest 
level by deleting an incorrect character and retyping.

A concept of an interface is central to layered network architecture. It is important 
to recognise implementation and design issues and pertaining to interfaces of layers 
and their respective functions and services. 

- entity: in software, it is sometimes called a process; in hardware in hardware it 
  could mean in pratice an I/O chip
- peer enties: entities at the same layer in different machines/devices
- service provider: eg. layer n, a layer that provides a service
- service receiver: eg. layer n + 1, a layer that receives a service
- service access point (SAP): a point where service is accessed, for example a 
  function call in software, or the telephone for a telephone company
- protocol data unit (PDU): a data unit that is communicated between peer entities
- service data unit (SDU): the PDU from the serice receiver
- protocol control information (PCI): is appended by a service receiver to an SDU 
  in order to indicated the type of service required and forms the IDU
- interface data unit (IDU): the data unit that is given from a service receiver to 
  a service provider.

The term "service" can be deemed to mean a number of things. These are outlined
below.

Quality of Service: each service is chractereised by a quality of service. For example, 
reliable service by such applications as file transfer - the data must be delivered 
correctly but it may be unusually delayed. However, voice and video transmission may 
allow some error in the data but does not allow delayed data.

Connection and connectionless services are the two fundamental categories. The 
distinctions between them may be intuitive but there are subtle differences. For 
example, a connection-orientated services allows two communicating parties to make 
use of the connection in any way they like - a telephone connection can even be 
used for transmitting fax and digital information as the service provider doesn't 
process the communication at any point through the network.

However a connectionless services does not provide such a convenience because 
for example a letter may be packaged and processed along the way, being stamped 
for accounting, delivered using a car or plane, etc....

A service is formally specified by a set of primitives (operations) that define
the service interface. The primitives differ for different services. As a simple 
example, a service may provide the following primitives:

1. LISTEN: listen for an incoming communication request
2. CONNECT: make a communication request
3. RECEIVE: receive data of a communication
4. SEND: send data of a communication
5. DISCONNECT: disconnect or discontinue a communication

As discussed before, each layer has specfic functions and offers certain services 
to the layer above it. A service is a set of primitives (operations) that a layer 
provides to the layer above it. In the definition of services, we do not specify 
their implementation. The implementation is only visible to the provider of the 
service.

A protocol defines the implementation of the service and is not visible to the 
user of the service. A protocol is a set of rules governing the format and 
meaning of the frames, packets, or messages within a layer and can be changed 
at will by entities, provided that they do not change the service visible to their 
users.

Attentuation is a physical phenomenon which causes a signal to gradually lose 
power as it propogates through a medium. This is mainly due to due to two 
factors, signal dispersion and signal absorption. Unfortunately, there is some 
noise which generates a constant noise power, N, and so eventually, as the 
signal travels a long enough distance, the signal poower will decrease to a 
value less than the noise power.

This is similar to the effect observed when you are attempting to communicate 
with another person with your natural voice. As the person moves further away 
the voice is harder to hear as the signal power spreads out and is absorbed 
by obstacles, etc....

Attenuation is generally a factor of distance and frequency.

This is very much in accordance with our current laws of physics.
The intensity if a signal is inversely proportional to the inverse square of the the signal distance. 
You'll often notice that in military applications the usage of broad 
frequency radar for spotting targets over great ranges and then the utilisation 
of higher frequency signals as greater precision positioning is required.

Most transmission medium will have a rating which gives the attenuation as a 
function of frequency and distance, eg. the attenuation per kilometer. Ideally 
the attenuation is zero because this represents no loss in the signal power.

This leads us to the issue of digital transmission versus analogue
transmission. The primary advantage of digital transmission over analogue 
transmission is the reduction in noise. The digital transmission system 
reduces noise over successive transmissions because small variations in 
the signal can be rounded off to the nearest level. Analogue transmission 
systems need to filter out the noise, but the filter itself can sometimes 
be a source of noise and generally, noise can accumulate to an unacceptable 
level in an analogue tranmission system.

Consider two types of repeater to understand of digital transmission. In the 
first instance (analogue) the situation is roughly synonymous with someone 
who doesn't know Korean and who is trying to relay a message from one Korean 
to another. It can be done if the person simply repeats the sounds that are 
heard. In digital tranmission repitition would be more along the lines of 
someone knowing Korean and actually "re-saying" what was heard (perhaps even 
correcting for errors in what was heard).

Systems which transmit ananlogue signals (such as the calssical radio and 
television broadcasting systems) can use one of three well known techniques,
 Amplitude Modulation (AM), Frequency Modulation (FM), and Phase Modulation (PM).
AM works by varying the amplitude of a signal to code the bit stream. FM works 
by varying frequency of a signal to code the bit stream. PM works by varying 
the phase of a signal to code the bit stream.

To allow two transmission medium (usually of the same type) to be directly 
connected the concept of a switch was developed. A general switch can be 
built from many basic switches and is somtimes referred to as a switching 
fabric. A lot of research and development contines in an attempt to build 
better switching fabrics.

A circuit switched network operates at the physical layer to provide a single, 
continuous transmission medium between any two hosts or end devices in the 
network. Circuit switching is perhaps the most straightforward approach to building 
communication network. Computer networks didn't actually evolve using circuit 
switching though circuit switching is being considered for certain computer 
applications such as video and voice. Because circuit switching constucts a 
single long transmission medium with at most analogue repeaters the end devices 
(in this case telephones) are able to modulate the signal on the medium however 
they choose. This is also why two modems can communicate with one another over 
a telephone network.

Multiplexing is used to share a single transmission medium among a number of 
individual transmissions. The demultiplexer just undoes what the multiplexer 
does. Therefore the multiplexer and demultiplexer must directly cooperate. 
Multiplexers and demultiplexers are not so much network devices are not so much 
network devices but functionality that is contained within a network device. 
Switches (in the sense described here) may be similarly categorized in this way.

The transmission medium can be multiplexed in the time domain, called TDM (Time Domain 
Multiplexing), using a simple switch and clocked control. Because of delays incurred 
in the transmission line, the clocks may need to be out of phase with one another. 
See that with the TDM, the shared medium must be able to transmit at a bit rate 
that equals (or is better than) the sum of input bit rates. The swithes must also 
be able to switch fast enough.

The previous example was assuming a synchronous transmission mode. In general, bit 
streams can be divided into DU's called cells. Cells are fixed in length so that the 
receiver knows when one cell finishes and the next cell begins.

Alternatively there is ATM (Asynchronous Transmission Mode, ATM is also the name 
given to a popular network layer device that uses this multiplexing technique.), 
where signals are irregularly multiplexed. How does the receiver know what order 
the signals were multiplexed in?

Additional information is needed in the multiplexed signal. One way is to add a 
header or label to the front of each cell. The header identifies the cell.

Using a header means using additional bits, which becomes overhead. Thus, to reduce 
the overhead the cells should be reasonably larger than the header. Cells for 
modern ATM transmission systems are about 50 butes in size.

Because modulators can transmit signals at different frequency bands, it is possible 
to transmit several signals at one, with each signal transmitted within a different 
frequency band. In this case the different signals have been frequency domain multiplexed.
This is known as FDM, (Frequency Domain Multiplexing).

Similar to FDM, WDM (Wavelength Division Multiplexing) uses each wavelength of light to 
transmit a signal. WDM is for fiber optic systems. The number of wavelengths per 
fiber, currently available, is about 300. Each wavelength can carry about 10Gbps. 
This makes 3Tbps. AT&T predict that up to 1024 wavelengths may be available in the future.

The hub/optical hub is clearly an analogue device - input signals are reflected to all 
outputs. It is also called passive, because it does not introduce any power to the signal 
(otherwise it would be called active). Considering the hub as a device which repeats 
input from an incoming signal on all outputs is quite simplistic however the basic 
function should be understandable. When examining the Data-link layer it will become 
clearer how computers communicate using a hub. For wireless communication, the 
transmission of each device is implicity received by every other device. This is an 
implicit hub.

Wireless communication usually takes place using the IEEE 802.11 standard which specifies 
an unlicensed radio spectrum at 2.4GHz and provides wireless Ethernet access at 11Mbps. 
There is also some spectrum allocated between 5GHz ad 6GHz that provides 54Mbps. 
Sophisticated codes and frequency sharing technology is used to maximise the usage of 
this communication medium. This topic is strictly concerned with Medium Access Control, 
to be discussed later. Wireless also allows point-to-point communication in the sense 
that each point-to-point link or channel is a dedicated frequency range that no other 
pair of devices will use.

Analogue telephone networks use conventional circuit switching networks to 
construct a connection between two telephones. You have seen that a modem 
converts digital signals to analogue to be transmitted over an analogue 
telephone network.

Digital telephone networks use a codec or coder-decoder to convert analogue 
telephone (or modem) signals into digital form! This is so that new digital 
techniques can be used within a conventionally an analogue network, ie. 
transparently to the analogue devices. The codec is used to convert from 
analogue to digital and digital to analogue.

Enough is known now to consider the complexity of a large digital network 
that supporst communication of various kinds. As technologies improve some 
networks become more dominant than others. In todays world we see a 
significant shift towards computer networks, away from conventional 
telecommunication networks.

This doesn't mean the end of telecommunications, but rather a collapse of 
outdated technological layers towards the "core" networks that they connect 
to. It will become more apparent to you as we progres through this subject 
that networks can be built on top of other networks which are themseleves 
built on top of ther networks.

The size and scale of a network can vary substantially from Local Area Networks 
(LANs) which usually connect 10-15 computers in a 
small area (usually a small building) to Metropolitan Area Networks (MANs)
which use long-distance links (such as telephone lines or dedicated media) 
to connect two or more locations within a single city or metropolitan area 
to Wide Area Networks (WANs) which use the same technology as MANs to 
connect computers in distant locations but in different cities 
or even different countries.

There are two basic types of networks: client-server networks, which use 
dedicated servers; and peer-to-peer networks, which (normally) share files 
between workstations. 

Server-based networks (also called client-server networks) use a dedicated 
server machine which provides files and printers to network workstations 
called clients. Client machines are simply used by network users, and usually 
do not share files or printers. The key benefit of the server-client model is 
in centralization. There is only a single point of control for network acccess, 
security, and management. The disadvantages of a server-based network are the 
higher cost of dedicated servers and network operating systems, as well as the 
increased level of administrative effort required.

Conversely, peer-to-peer networks consist solely of workstations called peers. 
Each workstation can be used by a user, and can also make shared files or 
printers available to users at other workstations. The advantages of peer 
networks include their ease of installation and use, their relative 
inexpensiveness in comparison with server-based networks (since a dedicated
server is not required) and the fact that an administrator mightn't be required 
(if users are able to manage resource sharing). The primary disadvantage of 
peer networks is the lack of a central control mechanism. Each user controls 
access to their own workstation's shared files and printers. In a large scale 
network network, such a security policy is difficult to manage without 
compromising security. Further, a workstation that is being accessed by peers
can also be slowed down, inconveniencing the user at that workstation. Hence, 
its clear that this system is best suited to smaller networks.

The basic idea behind this layering approach is to be able to abstract the 
entire process to make it easier to network, program and solve problems 
that may arise during networking. Each layer serves a specific purpose 
with the application layer being the most visible to the user (in terms of 
interfacing with the network) and physical layer being the least.

From a users persepective the impact of this approach is fairly nelgigible.
However, to the programmer it allows them to be able to add information to
the data being manipulated so that there is a lower chance of data
corruption as it travels through the network, so that transportation of
the data is sped up and also so that there is some flow control. To the
system administrator, it can form a valuable idea as to how to fix any
potential issues. For example, let's say that there seems to be a network
connection issue. The administrator can proceed in one of either two ways.
He can move upwards through the hierarchy or downwards, either way he is
eliminating possible issues at each point which therefore reduces the
possible number of problems at each point. The purpose of each layer will
be explained below.

</sect1>
